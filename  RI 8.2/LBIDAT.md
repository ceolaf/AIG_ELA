LBIDAT  
  
            The LBIDAT focuses on *Critical Issues* in the item draft, the kinds of problems that render and item unable to elicit evidence of the targeted cognition for the range of typical test takers without being prone to encouraging Type I and or Type II errors. Unlike the *Significant Issues* caused by problems with item hygiene, *Critical Issues* are not assumed to be fixable. Moreover, CDPs cannot be sure whether an issue is fixable until they attempt to do so++[1]++. Thus, the LBIDAT reports on the foundational and structural quality of an item, but not all of the refinements needed to actually appear on a test.   
            The LBIDAT reports can be treated as a rough guide to how much work an item requires before it is ready to be presented to a review committee, or perhaps even to a test developers’ client. Because addressing *Critical Issues* can beget more *Critical Issues*, its guidance is only rough. Moreover, even an item without *any* *Critical Issues* may still require hours of work to clean up all the *Significant Issues*.   
  
**The Scale**  
            The LBIDAT is a multi-dimensional, non-compensatory rubric. Users should evaluate each dimension independently, with one of three qualitative marks.  
·      **✓** The top mark, indicative of no *Critical Issues* in this area.  
·      ~ The middle mark, indicative of a low number of *Critical Issues* in this area, as might be expected initially.  
·      !! The lowest mark, indicative of a quite concerning number of *Critical Issues* in this area.  
            These marks can be reported together, but they cannot be added together or averaged, as this is a *non-compensatory* scale. Therefore, the summary of a an LBIDAT evaluation might look like one of the following, if they were to appear in a larger table of item metadata.  
            ~ ~ ~ ~ ~  
            **✓** ~ ~ !! ~  
            **✓ ✓ ✓** ~ !!  
  
**The Dimensions**  
            The LBIDAT considers five aspects of an item.  
            *The Stem & Task*. The stem of an item directs the test taker to engage in a task, promoting a cognitive path and response. This dimension primarily, though not exclusively, considers whether the task is appropriately aligned to the assigned alignment reference.   
            *The Key*. This dimension considers whether the key—the successful response—meets its requirements. As presented, the LBIDAT is focused on selected response items (e.g., multiple choice items), but this dimension can be modified to consider scoring guides for technology enhanced and constructed response items.  
            *Distractors*. This dimension considers unsuccessful responses, particularly in the context of selected response items. Because bad distractors can completely undermine the alignment of an otherwise appropriate task, this dimension is nearly as important to evaluating item alignment as the stem & task. The criteria can be modified to consider other kinds of unsuccessful responses for selected response, technology enhanced and constructed response items. Please not that a mismarked or unmarked key is not a *Critical Issue*, so long as the reviewer can identify the real key.  
            *Stimulus.* This dimension only examines item stimuli in the context of particular items and therefore takes a blindered view of stimuli issues. It should be evaluated again for each associated item that it is attached to. Thus, it considers whether the stimuli are appropriately/accurately described or linked by other stimuli elements of the item, and whether stimuli are properly constructed. For example, mislabeled graph axes or errors in a data table are such construction problems. As items are supposed to require analysis or manipulation of their stimuli to reach a successful response using the targeted cognition, items that may be answered by test takers without reference to their stimuli have a *Critical Issue *in this area.  
            *Fairness*. Fairness is a vast and vastly important topic. However, for LBIDAT purposes, this dimension only considers whether the stimulus and/or item raises inappropriate sensitivity topics. That is, whether it is likely to elicit an emotional response that notably distracts tests takers and thereby make them less likely to produce a successful response. In fact, this dimension should be evaluated through the lens of what the members of a fairness committee *for this assessment project* would conclude. That is, it is not a question of the what a CDP thinks really would or ought to disturb some test takers, but rather how the CDP expects a fairness committee to rule on this issue. This often requires experience with the political and/or empathetic sensitivities of a particular test sponsor/client and the kinds of people they want on their fairness review committees. (Issues that give some groups inappropriate advantage (i.e., bias) over others in the testing populations are perhaps the most important *Significant Issues* and can be the most challenging to address. However, because CDPs cannot always spot them themselves, they fall *outside* of the LBIDAT’s criteria.  
  
**Critical Issues**  
            The LBIDAT focuses exclusively on *Critical Issues*. This excludes almost all item hygiene issues and even many of the content & cognition traits of high quality items. Those may be *Significant Issues*, but they are issues for later steps in item development. *Critical Issues* in an item are those that may absolutely prevent an item from being usable, because they so undermine an item’s ability to elicit evidence of the targeted cognition for the range of typical test takers. *Critical Issues* may be fixable, but it is often unclear until a CDP attempts to address them whether or not that is the case. Unfortunately, fixing one Critical Issue can create a new—perhaps even more than one—critical issue. Therefore, one cannot simply sum up the number of *Critical Issues* in an item.  
            The *Critical Issues* that should be considered for each dimension listed on the LBIDAT form (see next page). Of course, any given project may alter those lists, but other issues, be they item hygiene (Wine & Hoffman, 2022), content & cognition traits (Wine, Glore & Hoffman, 2025) or anything else should be identifiable by CDPs without the support of expert external review committee members.   
              
**The Rubric**  
  
*Stem & Task*  

| No Apparent 
Critical Issues | 1 Apparent 
Critical Issues | 2+ Apparent
Critical Issues |  |
| ---------------------------- | --------------------------- | --------------------------- | - |
| ✓ | ~ | !! |  |
  
·       The prompted task is based upon a misunderstanding of the targeted cognition.  
·       The prompted task is based upon a misunderstanding or misreading of its own stimulus or the work it attempts to prompt.   
·       The targeted cognition is not at a (grade) level appropriate version of the cognition.   
·       The targeted cognition is not part of the most important core of the alignment reference or standard.   
·       Alternative Paths: The prompted task does not depend upon the targeted cognition for a significant number of typical test takers.  
·       Additional KSA: The prompted task also requires some other KSAs outside of the alignment reference that are at the item’s (grade) level, above the item’s (grade) level, or just one (grade) level below the item.  
·       The task requires notable learning for a successful response. (May be acceptable for inquiry-based tasks aligned to inquiry-based alignment references.)  
  
*The Key*  

| No Apparent 
Critical Issues | 1+ Apparent 
Critical Issues |  |
| ---------------------------- | ---------------------------- | - |
| ✓                            | !!                           |  |
  
·       The key is not directly responsive to the question or command in the stem.   
·       The key is not *definitively* correct.  
  
*Distractors*  

| No Apparent 
Critical Issues | 1-2 Apparent 
Critical Issues | 3+ Apparent
Critical Issues |  |
| ---------------------------- | ----------------------------- | --------------------------- | - |
| ✓ | ~ | !! |  |
  
·       Each distractor that does not appear to directly respond to the question or command in the stem.  
·       Each distractor that is not the product of an error, misapplication or misconception with the targeted cognition (i.e., is plausible)  
·       Each distractor that is not *definitively* incorrect.   
·       Multiple distractors follow from the same error, misapplication or misconception as another distractor.  
·       Each distractor that is an exact duplicate of another distractor.  
  
*Stimulus*  

| No Apparent 
Critical Issues | 1-2 Apparent 
Critical Issues | 3+ Apparent
Critical Issues |  |
| ---------------------------- | ----------------------------- | --------------------------- | - |
| ✓ | ~ | !! |  |
  
·       The stimulus requires too much time for test takers to make sense of. (Primarily for stand-alone items.)  
·       The stimulus contains inappropriately implausible or incorrect elements.  
·       The item does not require the stimulus for a successful response for a significant number of typical test takers.  
·       Each element of the stimulus that does not match its description or assumptions elsewhere  
·       Each construction mistake in structured stimuli.  
  
*Fairness*  

| No Apparent 
Critical Issues | 1+ Apparent 
Critical Issues |  |
| ---------------------------- | ---------------------------- | - |
| ✓                            | !!                           |  |
  
·       Each inappropriate sensitivity topic raised by the item  
  
  
++[1]++ Even relatively inexperienced CDPs may have a good sense that a *Critical Issue* in an item is not fixable and experienced CDPs may have a good sense that a particular Critical Issue is fixable—or even quickly and easily fixable. But these senses of a fixability of a *Critical Issue* are not final determinations. Any of them can prove wrong after serious attempts to actually fix them, though of course that is less likely with more experienced CDPs.   
